{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/glove/blob/master/Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoMjuOxSg-cP",
        "colab_type": "code",
        "outputId": "1d7d6e9d-c038-4d71-e9a9-9496800ad422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxq9uB5rhSdE",
        "colab_type": "code",
        "outputId": "455e1240-7527-44d9-a941-9a08b048891e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Glove\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Glove\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mBA_AjIiDYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from w2v_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxHQk_4FivWk",
        "colab_type": "text"
      },
      "source": [
        "Read data, the GloVe 50d vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCYUhtgDiyPf",
        "colab_type": "code",
        "outputId": "ef08a9f5-3310-467f-8fc1-5bc82a85e65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Words is the set of words of the \"English vocabulary\". word_to_vec_map is the char_to_ix of the previous week. It's the vector form of the vocabulary.\n",
        "words, word_to_vec_map = read_glove_vecs(\"glove.6B.50d.txt\")\n",
        "print(\"Everything is read\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything is read\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y298AozijePy",
        "colab_type": "text"
      },
      "source": [
        "One of the key concepts is the cosine similarity. This is a scalar product in algebra. 2 vectors in a vector space will be the same if their cosine is 1 and they will be opposite when their cosine is -1. At 0 they are ortogonal. It's just the definition of scalar product.\n",
        "\n",
        "$cos \\theta = \\frac{\\overrightarrow{u} \\cdot \\overrightarrow{v}}{|\\overrightarrow{u}| \\cdot |\\overrightarrow{v}|}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WgHevxwkT4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    This function takes the two vectors u and v and calculates the cosine of the angle between them.\n",
        "    \n",
        "    u, v are vectors of shape (n,)\n",
        "    \n",
        "    returns cosine which is a scalar   \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    dot = np.dot(u, v)\n",
        "    \n",
        "    u_norm = np.linalg.norm(u, ord = 2)\n",
        "    v_norm = np.linalg.norm(v, ord = 2)\n",
        "    \n",
        "    cosine = dot / (u_norm * v_norm)\n",
        "    \n",
        "    return cosine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn4xUsv4z_-3",
        "colab_type": "code",
        "outputId": "8ae72e62-3367-4af2-944a-72a111b5262d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "crocodile = word_to_vec_map[\"crocodile\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "print(\"Similarity father/mother = \", cosine_similarity(father, mother))\n",
        "print(\"Similarity ball/crocodile = \", cosine_similarity(ball, crocodile))\n",
        "print(\"Similarity france - paris / italy - rome = \", cosine_similarity(france - paris, italy - rome))\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity father/mother =  0.8909038442893615\n",
            "Similarity ball/crocodile =  0.2743924626137942\n",
            "Similarity france - paris / italy - rome =  0.6751479308174202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwSgMpAw1Hn7",
        "colab_type": "text"
      },
      "source": [
        "Now I will do the word analogy. Namely, check if $e_a - e_b \\approx e_c - e_d$. I will obviously use the cosine similarity for this. The input are the first three words/vectors and I will loop through all the vocabulary to obtain the most similar word to the 3rd one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVgiQLNP5Bx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    This function takes 3 words and the mapping to find the 4th word in an analogy\n",
        "    a is to b as c is to...\n",
        "    \n",
        "    output is a word    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    word_a = word_a.lower() #In case the user didn't do it\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "    \n",
        "    # Get the vectors\n",
        "    \n",
        "    e_a = word_to_vec_map[word_a]\n",
        "    e_b = word_to_vec_map[word_b]\n",
        "    e_c = word_to_vec_map[word_c]\n",
        "    \n",
        "    words = word_to_vec_map.keys() # This is my list of words\n",
        "    \n",
        "    max_cos_similarity = -1e3\n",
        "    best_word = None\n",
        "    \n",
        "    # Go over the whole set, detect if I am choosing the same word\n",
        "    for w in words:\n",
        "        if w in [word_a, word_b, word_c]:\n",
        "            continue\n",
        "        else:\n",
        "            cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
        "            \n",
        "            if cosine_sim > max_cos_similarity:\n",
        "                max_cos_similarity = cosine_sim\n",
        "                best_word = w\n",
        "                \n",
        "    return best_word\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tM42uhm63Fl",
        "colab_type": "code",
        "outputId": "62620be6-0ec4-4a19-c1cc-559671a79ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "triads = [(\"italy\", \"italian\", \"spain\"), (\"paris\", \"france\", \"Madrid\"), (\"man\", \"woman\", \"boy\"), (\"small\", \"smaller\", \"huge\")]\n",
        "\n",
        "for triad in triads:\n",
        "    print(\"{} is to {} as {} is to {}\".format(*triad, complete_analogy(*triad, word_to_vec_map)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "italy is to italian as spain is to spanish\n",
            "paris is to france as Madrid is to spain\n",
            "man is to woman as boy is to girl\n",
            "small is to smaller as huge is to revenues\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwQc_Qa-8rgw",
        "colab_type": "text"
      },
      "source": [
        "The last one is funny. It's also funny how if I change the order of the second one it finds aires instead of Madrid. I guess it's from Buenos Aires. Damn argentinians."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iPWCJ1Y9i_X",
        "colab_type": "text"
      },
      "source": [
        "Now let's do the debiasing. In english this is quite straightforward. In a given base, in which one of the axes is gender, father and mother should be symmetrical respect to this axis. Another way of seeing this, a neutral word like doctor, should be one of the axes of symmetry between man and woman. This may not always be the case due to biases that may be present in the corpus. Let's see this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RlaUaP_K8C",
        "colab_type": "code",
        "outputId": "fd2712a5-c702-42c6-f89b-e8e566472ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "g = word_to_vec_map[\"man\"] - word_to_vec_map[\"woman\"] # This is the gender axis\n",
        "\n",
        "print(g)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.087144   -0.2182      0.40986     0.03922     0.1032     -0.94165\n",
            "  0.06042    -0.32988    -0.46144     0.35962    -0.31102     0.86824\n",
            " -0.96006    -0.01073    -0.24337    -0.08193     1.02722     0.21122\n",
            " -0.695044    0.00222    -0.29106    -0.5053      0.099454   -0.40445\n",
            " -0.30181    -0.1355      0.0606      0.07131     0.19245     0.06115\n",
            "  0.3204     -0.07165     0.13337     0.25068714  0.14293     0.224957\n",
            "  0.149      -0.048882   -0.12191     0.27362     0.165476    0.20426\n",
            " -0.54376     0.271425    0.10245     0.32108    -0.2516      0.33455\n",
            "  0.04371    -0.01258   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-T0-3M_cDH",
        "colab_type": "text"
      },
      "source": [
        "Now compute the similarity between male names and g and between female names and g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL6tNX4__hVg",
        "colab_type": "code",
        "outputId": "d0984e90-51ab-473b-982c-94b68b6df8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "names = [\"alex\", \"john\", \"charlie\", \"mary\", \"kelly\", \"katy\"]\n",
        "\n",
        "for w in names:\n",
        "    print(w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alex 0.1289169612113885\n",
            "john 0.23163356145973724\n",
            "charlie 0.15980474924204568\n",
            "mary -0.2442842158649063\n",
            "kelly 0.040351556806825936\n",
            "katy -0.2831068659572615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdcChqeALO8",
        "colab_type": "text"
      },
      "source": [
        "Male names tend to be positive and female names tend to be negative. This makes sense and there is no obvious problem. Let's see with other words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_UnnTxlAVFf",
        "colab_type": "code",
        "outputId": "2cb675b8-085a-4755-df93-b400f564970c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "words = [\"gun\", \"lipstick\", \"truck\", \"baby\", \"motorbike\", \"science\", \"arts\"]\n",
        "\n",
        "for w in words:\n",
        "    print(w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gun 0.1172836185493845\n",
            "lipstick -0.2769191625638267\n",
            "truck 0.032396832512384365\n",
            "baby -0.1925003258097509\n",
            "motorbike -0.12532685677161912\n",
            "science 0.06082906540929701\n",
            "arts -0.008189312385880337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z46cxEazAlZa",
        "colab_type": "text"
      },
      "source": [
        "There are some words that should be neutral (close to 0, perpendicular to the axis) that are not. Neutralize them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnqyQrOv-VLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    This function takes a word and an axis (gender in this case), and produces the new coordinates\n",
        "    \n",
        "    input: a word, the axis and the mapping.\n",
        "    \n",
        "    output: the new vector debiased\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    e = word_to_vec_map[word]\n",
        "    \n",
        "    # Projection of a vector on a given axis!\n",
        "    e_axis_component = np.dot(e, g)/np.dot(g, g) * g\n",
        "    \n",
        "    e_debiased = e - e_axis_component\n",
        "    \n",
        "    return e_debiased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QRxvGVFFOxq",
        "colab_type": "code",
        "outputId": "65d7704a-59de-4462-9e7b-eae6a6f6fdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "e = \"receptionist\"\n",
        "\n",
        "print(\"Cosine similarity between \" + e + \" and g before neutralizing\", cosine_similarity(word_to_vec_map[e], g))\n",
        "e_deb = neutralize(e, g, word_to_vec_map) #This is already a vector!!\n",
        "print(\"Cosine similarity between \" + e + \" and g after neutralizing\", cosine_similarity(e_deb, g))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between receptionist and g before neutralizing -0.3307794175059374\n",
            "Cosine similarity between receptionist and g after neutralizing 2.6832242276243644e-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eKAiMrSGSJ6",
        "colab_type": "text"
      },
      "source": [
        "Finally, sex specific words like actor and actress should be equalized. In theory actor is as masculine as actress is feminine, but it may not be this case. The derivation of this is [here](https://arxiv.org/abs/1607.06520)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DaEytXaGeHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def equalize(pair, axis, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    This function takes a pair of words and an axis and equalizes the projection.\n",
        "    \n",
        "    input: pair of words, axis g (50 d) and mapping\n",
        "    \n",
        "    output: e1 and e2, the two new vectors\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    w1 = pair[0]\n",
        "    w2 = pair[1]\n",
        "    \n",
        "    e_1 = word_to_vec_map[w1]\n",
        "    e_2 = word_to_vec_map[w2]\n",
        "    \n",
        "    mu = 0.5 * (e_1 + e_2)\n",
        "    \n",
        "    # Projection of e1 and e2 on mu and mu_orthogonal\n",
        "    \n",
        "    mu_B = np.dot(mu, g) * g / np.linalg.norm(g, ord = 2)**2\n",
        "    mu_orth = mu - mu_B\n",
        "    \n",
        "    e_1B = np.dot(e_1, g) * g / np.linalg.norm(g, ord = 2)**2\n",
        "    e_2B = np.dot(e_2, g) * g / np.linalg.norm(g, ord = 2)**2\n",
        "    \n",
        "    e_1_new = e_1B - mu_B + mu_orth\n",
        "    e_2_new = e_2B - mu_B + mu_orth\n",
        "    \n",
        "    return e_1_new, e_2_new\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0qCYOniLHVV",
        "colab_type": "code",
        "outputId": "ff83bdc7-28d1-4ace-8ff6-22993fe8cc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(\"Cos similarities before equalizing: \")\n",
        "print(\"Cos similarity between man and g = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
        "print(\"Cos similarity between woman and g = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
        "print(\"Cos similarities after equalizing: \")\n",
        "\n",
        "man_2, woman_2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "\n",
        "print(\"Cos similarity between man and g = \", cosine_similarity(man_2, g))\n",
        "print(\"Cos similarity between woman and g = \", cosine_similarity(woman_2, g))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cos similarities before equalizing: \n",
            "Cos similarity between man and g =  0.11711095765336832\n",
            "Cos similarity between woman and g =  -0.35666618846270376\n",
            "Cos similarities after equalizing: \n",
            "Cos similarity between man and g =  0.24239737572313433\n",
            "Cos similarity between woman and g =  -0.2423973757231343\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}